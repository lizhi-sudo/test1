{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3620034",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<center> <h1> Exploiting Asymmetry for Synthetic Training Data Generation: <br>SynthIE and the Case of Information Extraction </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9175c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8992b0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Table of Content\n",
    "1. [Data](#Data)<br>\n",
    "    1.1. [Direct Download and Loading](#Direct-Download-and-Loading) (required for running code in the repository)<br>\n",
    "    1.2. [HuggingFace Datasets Download and Loading](#HuggingFace-Datasets-Download-and-Loading) (only for providing access to the data through HuggingFace)\n",
    "\n",
    "2. [Models and Inference](#Models-and-Inference)<br>\n",
    "    2.1 [Model Download](#Model-Download)<br>\n",
    "    2.2 [Model Loading](#Model-Loading)<br>\n",
    "    2.3 [Unconstrained Decoding](#Unconstrained-Decoding)<br>\n",
    "    2.4 [Constrained Decoding](#Constrained-Decoding)<br>\n",
    "\n",
    "3. [Loading Models and Datasets with Hydra](#Loading-Models-and-Datasets-with-Hydra)\n",
    "\n",
    "4. [Loading and Using the WikidataID2Name Dictionaries](#Loading-Models-and-Datasets-with-Hydra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee45736",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4fe12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5e702",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The HuggingFace [dataset card](https://huggingface.co/datasets/martinjosifoski/SynthIE) contains information about the available datasets and data splits, a detailed description of the fields, and some basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6719ff8-c0f2-4cde-90c0-46726e8eeb9d",
   "metadata": {},
   "source": [
    "### Direct Download and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7999e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code in the repository assumes that all the necessary data is in the `data` directory, with different parts of the code relying on different data. To download all the data in the `data` directory (around 4.4GB), from the project's root directory, execute the following code:\n",
    "\n",
    "```bash download_data.sh```\n",
    "\n",
    "If you want to omit some files, comment out the specific lines in the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6eab2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a different directory for your data, update the path below\n",
    "DATA_DIR=\"../data\"\n",
    "\n",
    "# To download the data, uncomment the following line and run the cell\n",
    "# !bash ../download_data.sh $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fa92f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import jsonlines\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d32bfa-5f63-4915-91d2-8e8384acbbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name2folder_name = {\n",
    "    \"synthie_text\": \"sdg_text_davinci_003\",\n",
    "    \"synthie_text_pc\": \"sdg_text_davinci_003\",\n",
    "    \"synthie_code\": \"sdg_code_davinci_002\", \n",
    "    \"synthie_code_pc\": \"sdg_code_davinci_002\", \n",
    "    \"rebel\": \"rebel\",\n",
    "    \"rebel_pc\": \"rebel\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_full_path(data_dir, dataset_name, split):\n",
    "    file_name = f\"{split}.jsonl.gz\"\n",
    "    \n",
    "    if dataset_name.endswith(\"_pc\"):\n",
    "        data_dir = os.path.join(data_dir, \"processed\")\n",
    "        if not dataset_name.startswith(\"rebel\"):\n",
    "            file_name = f\"{split}_ordered.jsonl.gz\"\n",
    "    \n",
    "    return os.path.join(data_dir, dataset_name2folder_name[dataset_name], file_name)\n",
    "\n",
    "def read_gzipped_jsonlines(path_to_file):\n",
    "    with gzip.open(path_to_file, \"r\") as fp:\n",
    "        json_reader = jsonlines.Reader(fp)\n",
    "        data = list(json_reader)\n",
    "    return data\n",
    "\n",
    "dataset_name = 'synthie_text' # 'synthie_code', 'rebel', 'synthie_text_pc', 'synthie_code_pc', 'rebel_pc'\n",
    "split = \"test\" # \"train\", \"test\", \"test_small\" \n",
    "\n",
    "path_to_file = get_full_path(DATA_DIR, dataset_name, split)\n",
    "data = read_gzipped_jsonlines(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5971f2",
   "metadata": {},
   "source": [
    "### HuggingFace Datasets Download and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0be45da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset synth_ie (/home/martin/.cache/huggingface/datasets/martinjosifoski___synth_ie/synthie_text/1.0.0/dafe24bbede03960f5e57caca7e56d3dc917919ba1161566607366ef74a4b52b)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_name = 'synthie_text' # 'synthie_code', 'rebel', 'synthie_text_pc', 'synthie_code_pc', 'rebel_pc'\n",
    "split = \"test\" # \"train\", \"test\", \"test_small\"\n",
    "\n",
    "dataset = datasets.load_dataset(f\"martinjosifoski/SynthIE\", dataset_name, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affd3af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Models and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736b73c",
   "metadata": {},
   "source": [
    "### Model Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a353087",
   "metadata": {},
   "source": [
    "For a list and a description of the provided models see the HuggingFace [model card](https://huggingface.co/martinjosifoski/SynthIE). For more details about the models, please refer to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1475d75",
   "metadata": {},
   "source": [
    "To download all the data in the `data/models` directory (around ToDO), from the project's root directory, execute the following code:\n",
    "\n",
    "```bash download_models.sh```\n",
    "\n",
    "If you want to omit some models, comment out the specific lines in the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed8ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a different directory for your data, update the path below\n",
    "MODELS_DIR=\"../data/models\"\n",
    "\n",
    "# To download the models, uncomment the following line and run the cell\n",
    "# !bash ../download_models.sh $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5359b51",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137ce36d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the Model (downloaded in the ../data/models directory)\"\"\"\n",
    "from src.models import GenIEFlanT5PL\n",
    "\n",
    "ckpt_name = \"synthie_base_sc.ckpt\"\n",
    "path_to_checkpoint = os.path.join(DATA_DIR, 'models', ckpt_name)\n",
    "model = GenIEFlanT5PL.load_from_checkpoint(checkpoint_path=path_to_checkpoint)\n",
    "model.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36cef20",
   "metadata": {},
   "source": [
    "For inference use the `model.sample` function. \n",
    "\n",
    "Under the hood, **SynthIE** uses the HuggingFace's generate function, thus it accepts the same generation parameters. The model's default generation parameters can be overriden as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd925279",
   "metadata": {},
   "outputs": [],
   "source": [
    "override_models_default_hf_generation_parameters = {\n",
    "    \"num_beams\": 10,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"seed\": 123,\n",
    "    \"length_penalty\": 0.8\n",
    "}\n",
    "\n",
    "texts = ['The Journal of Colloid and Interface Science is a bibliographic review indexed in Scopus and published by Elsevier. Its main subject is chemical engineering, and it is written in the English language. It is based in the United States, and is owned by Elsevier, the same company that owns Scopus.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c994d7e",
   "metadata": {},
   "source": [
    "### Unconstrained Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acc04e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('Journal_of_Colloid_and_Interface_Science',\n",
       "   'country of origin',\n",
       "   'United_States'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'indexed in bibliographic review',\n",
       "   'Scopus'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'language of work or name',\n",
       "   'English_language'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'main subject',\n",
       "   'Chemical_engineering'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science', 'publisher', 'Elsevier'),\n",
       "  ('Scopus', 'owned by', 'Elsevier')}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.sample(texts,\n",
    "                      convert_to_triplets=True,\n",
    "                      return_generation_outputs=True,\n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output['grouped_decoded_outputs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02448d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name:\n",
      "[{('Journal_of_Colloid_and_Interface_Science',\n",
      "   'country of origin',\n",
      "   'United_States'),\n",
      "  ('Journal_of_Colloid_and_Interface_Science',\n",
      "   'indexed in bibliographic review',\n",
      "   'Scopus'),\n",
      "  ('Journal_of_Colloid_and_Interface_Science',\n",
      "   'language of work or name',\n",
      "   'English_language'),\n",
      "  ('Journal_of_Colloid_and_Interface_Science',\n",
      "   'main subject',\n",
      "   'Chemical_engineering'),\n",
      "  ('Journal_of_Colloid_and_Interface_Science', 'publisher', 'Elsevier'),\n",
      "  ('Scopus', 'owned by', 'Elsevier')}]\n",
      "['[s] Journal_of_Colloid_and_Interface_Science [r] indexed in bibliographic '\n",
      " 'review [o] Scopus [r] publisher [o] Elsevier [r] main subject [o] '\n",
      " 'Chemical_engineering [r] language of work or name [o] English_language [r] '\n",
      " 'country of origin [o] United_States [e] [s] Scopus [r] owned by [o] Elsevier '\n",
      " '[e]']\n"
     ]
    }
   ],
   "source": [
    "# ~~ synthie_base_sc\n",
    "# ~~ Load model ~~\n",
    "from src.models import GenIEFlanT5PL\n",
    "\n",
    "from pprint import pprint \n",
    "\n",
    "ckpt_name = \"synthie_base_sc.ckpt\"\n",
    "path_to_checkpoint = os.path.join(DATA_DIR, 'models', ckpt_name)\n",
    "model = GenIEFlanT5PL.load_from_checkpoint(checkpoint_path=path_to_checkpoint)\n",
    "model.to(\"cuda\");\n",
    "\n",
    "output = model.sample(texts,\n",
    "                      convert_to_triplets=True,\n",
    "                      return_generation_outputs=True,\n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "print(\"Checkpoint name:\")\n",
    "pprint(output['grouped_decoded_outputs'][0])\n",
    "pprint(model.tokenizer.batch_decode(\n",
    "    output['generation_outputs'].sequences, skip_special_tokens=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25281ef",
   "metadata": {},
   "source": [
    "### Constrained Decoding \n",
    "\n",
    "Assumes that the `constrained_world` definitions have been downloaded using the `download_data.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab68d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load constrained decoding module\"\"\"\n",
    "from src.constrained_generation import IEConstrainedGeneration\n",
    "\n",
    "params = {}\n",
    "params['constrained_worlds_dir'] = os.path.join(DATA_DIR, \"constrained_worlds\")\n",
    "params['constrained_world_id'] = \"genie_t5_tokenizeable\" # specifies the folder name from which the constrained world is loaded\n",
    "params['identifier'] = \"genie_t5_tokenizeable\" # specifies the cache subfolder where the trie will be stored\n",
    "    \n",
    "params['path_to_trie_cache_dir'] = os.path.join(DATA_DIR, \".cache\")\n",
    "params['path_to_entid2name_mapping'] = os.path.join(DATA_DIR, \"id2name_mappings\", \"entity_mapping.jsonl\")\n",
    "params['path_to_relid2name_mapping'] = os.path.join(DATA_DIR, \"id2name_mappings\", \"relation_mapping.jsonl\")\n",
    "\n",
    "constraint_module = IEConstrainedGeneration.from_constrained_world(model=model, \n",
    "                                                                   linearization_class_id=model.hparams.linearization_class_id, \n",
    "                                                                   **params)\n",
    "\n",
    "model.constraint_module = constraint_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f3d585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('Journal_of_Colloid_and_Interface_Science',\n",
       "   'country of origin',\n",
       "   'United_States'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'indexed in bibliographic review',\n",
       "   'Scopus'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'language of work or name',\n",
       "   'English_language'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'main subject',\n",
       "   'Chemical_engineering'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science', 'publisher', 'Elsevier'),\n",
       "  ('Scopus', 'owned by', 'Elsevier')}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.sample(texts,\n",
    "                      convert_to_triplets=True,\n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output['grouped_decoded_outputs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a1f12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955a215",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Loading Models and Datasets with Hydra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb1954",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An alternative way to load the models (or the data) is by using the package manage [Hydra](https://hydra.cc/). Below we provide an example of using Hydra in a jupyter notebook but the library really shines when used in scripts. See our training and evaluation pipelines, outlined in the README, for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a05252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the data from: ../data/processed/sdg_text_davinci_003/val_ordered.jsonl.gz: 100%|█| 1\n"
     ]
    }
   ],
   "source": [
    "# ~~~Load config~~~\n",
    "import hydra\n",
    "\n",
    "configs_path = \"../configs\"\n",
    "config_name = \"inference_root.yaml\"\n",
    "model_id = \"synthie_base_sc\"\n",
    "dataset = \"sdg_text_davinci_003_pc\"\n",
    "\n",
    "with hydra.initialize(version_base=\"1.2\", config_path=configs_path):\n",
    "    cfg = hydra.compose(config_name=config_name, \n",
    "                           overrides=[f\"data_dir={DATA_DIR}\",\n",
    "                                      f\"work_dir=../\",\n",
    "                                      f\"+experiment/inference={model_id}\",\n",
    "                                      f\"datamodule={dataset}\"\n",
    "                                     ])\n",
    "    \n",
    "# ~~~Load model~~~\n",
    "model = hydra.utils.instantiate(cfg.model, _recursive_=False)\n",
    "model.to(\"cuda\");\n",
    "\n",
    "# ~~~Load dataset~~~\n",
    "datamodule = hydra.utils.instantiate(cfg.datamodule, _recursive_=False)\n",
    "datamodule.set_tokenizer(model.tokenizer)\n",
    "datamodule.setup(\"validate\")\n",
    "datamodule.set_tokenizer(model.tokenizer)\n",
    "\n",
    "# If defined, use the model's collate function (otherwise proceed with the PyTorch's default collate_fn)\n",
    "if getattr(model, \"collator\", None):\n",
    "    datamodule.set_collate_fn(model.collator.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24ffbdca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "override_models_default_hf_generation_parameters = {\n",
    "    \"num_beams\": 10,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"seed\": 123,\n",
    "    \"length_penalty\": 0.8\n",
    "}\n",
    "\n",
    "texts = [datamodule.data_val[1]['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704e45a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('Journal_of_Colloid_and_Interface_Science',\n",
       "   'country of origin',\n",
       "   'United_States'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'indexed in bibliographic review',\n",
       "   'Scopus'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'language of work or name',\n",
       "   'English_language'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science',\n",
       "   'main subject',\n",
       "   'Chemical_engineering'),\n",
       "  ('Journal_of_Colloid_and_Interface_Science', 'publisher', 'Elsevier'),\n",
       "  ('Scopus', 'owned by', 'Elsevier')}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.sample(texts,\n",
    "                      convert_to_triplets=True,\n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output['grouped_decoded_outputs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30f4b3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Loading and Using the WikidataID2Name Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2687759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_entity_id2name_mapping = os.path.join(DATA_DIR, \"id2name_mappings\", \"entity_mapping.jsonl\")\n",
    "with jsonlines.open(path_to_entity_id2name_mapping) as reader:\n",
    "    entity_id2name_mapping = {obj[\"id\"]: obj[\"en_label\"] for obj in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baae4dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q191069', 'Paleoclimatology'),\n",
       " ('Q47716', 'Charleston,_South_Carolina'),\n",
       " ('Q15643', 'Juliet_(moon)'),\n",
       " ('Q2143143', 'Owaneco,_Illinois'),\n",
       " ('Q1899035', 'Erhards_Grove_Township,_Otter_Tail_County,_Minnesota')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_id2name_mapping.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c0a16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_relation_id2name_mapping = os.path.join(DATA_DIR, \"id2name_mappings\", \"relation_mapping.jsonl\")\n",
    "with jsonlines.open(path_to_relation_id2name_mapping) as reader:\n",
    "    relation_id2name_mapping = {obj[\"id\"]: obj[\"en_label\"] for obj in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98101f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P2673', 'next crossing upstream'),\n",
       " ('P571', 'inception'),\n",
       " ('P186', 'material used'),\n",
       " ('P1066', 'student of'),\n",
       " ('P453', 'character role')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(relation_id2name_mapping.items())[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlr",
   "language": "python",
   "name": "mlr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
